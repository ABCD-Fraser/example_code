{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction, analysis and visualisation for IMBES conference \n",
    "\n",
    "This code was put together to share with my supervisor to extract EEG data to present at a conference this summer. \n",
    "\n",
    "Participants completed  ~120 trials. This trial takes the EEG associated with each trial and averages accross multiple experimental conditions. It then visualises the data to select the key time points for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup file\n",
    "\n",
    "Import files and set up custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from scipy.signal import find_peaks\n",
    "import pingouin as pg\n",
    "\n",
    "import mne\n",
    "\n",
    "# Function to update event codes\n",
    "def update_events(raw):\n",
    "    events, labels = mne.events_from_annotations(raw, verbose='Warning')\n",
    "\n",
    "    target_onset_trigger = labels['Stimulus/target_onset']\n",
    "    trigger_1 = labels['Stimulus/S  1'] if 'Stimulus/S  1' in labels else '199'\n",
    "    trigger_2 = labels['Stimulus/S  2'] if 'Stimulus/S  2' in labels else '299'\n",
    "    trigger_3 = labels['Stimulus/S  3'] if 'Stimulus/S  3' in labels else '399'\n",
    "    trigger_5 = labels['Stimulus/S  5'] if 'Stimulus/S  5' in labels else '599'\n",
    "    trigger_6 = labels['Stimulus/S  6'] if 'Stimulus/S  6' in labels else '699'\n",
    "    trigger_7 = labels['Stimulus/S  7'] if 'Stimulus/S  7' in labels else '799'\n",
    "\n",
    "    print(trigger_1, trigger_2, trigger_3, trigger_5, trigger_6, trigger_7)\n",
    "\n",
    "    num_trig = [trigger_1, trigger_2, trigger_3, trigger_5, trigger_6, trigger_7]\n",
    "\n",
    "    for i in range(len(events)):\n",
    "        if events[i, 2] == target_onset_trigger:\n",
    "            j = i\n",
    "            while True:\n",
    "                j += 1\n",
    "                if events[j, 2] in num_trig:\n",
    "                    if events[j, 2] in [trigger_1, trigger_7]:\n",
    "                        events[i, 2] = 300\n",
    "                    elif events[j, 2] in [trigger_2, trigger_6]:\n",
    "                        events[i, 2] = 200\n",
    "                    elif events[j, 2] in [trigger_3, trigger_5]:\n",
    "                        events[i, 2] = 100\n",
    "                    break\n",
    "\n",
    "    if 100 in events[:,2]:\n",
    "        labels['Stimulus/target_close'] = 100\n",
    "    if 200 in events[:,2]:    \n",
    "        labels['Stimulus/target_mid'] = 200\n",
    "    if 300 in events[:,2]:\n",
    "        labels['Stimulus/target_far'] = 300\n",
    "\n",
    "    return events, labels\n",
    "\n",
    "\n",
    "\n",
    "    # Function to detect local peaks using find_peaks\n",
    "def detect_peaks(evoked, tmin_n1=None, tmax_n1=None, tmin_p2=None, tmax_p2=None, tmin_p3b=None, tmax_p3b=None, peak_to_detect=None):\n",
    "    times = evoked.times\n",
    "    data = evoked.data[0, :]\n",
    "    \n",
    "    if peak_to_detect == 'N1':\n",
    "        # Detect local minima (N1) within the specified time range\n",
    "        idx_n1_window = np.where((times >= tmin_n1) & (times <= tmax_n1))[0]\n",
    "        data_n1_window = data[idx_n1_window]\n",
    "        n1_peaks, _ = find_peaks(-data_n1_window)  # Negate data to find minima\n",
    "        if n1_peaks.size > 0:\n",
    "            n1_peak_idx = idx_n1_window[n1_peaks[0]]\n",
    "            n1_latency = times[n1_peak_idx]\n",
    "            n1_amplitude = data[n1_peak_idx]\n",
    "        else:\n",
    "            n1_latency, n1_amplitude = np.nan, np.nan\n",
    "\n",
    "        return n1_latency, n1_amplitude\n",
    "\n",
    "    elif peak_to_detect == 'P2':\n",
    "        # Detect local maxima (P2) within the specified time range\n",
    "        idx_p2_window = np.where((times >= tmin_p2) & (times <= tmax_p2))[0]\n",
    "        data_p2_window = data[idx_p2_window]\n",
    "        p2_peaks, _ = find_peaks(data_p2_window)\n",
    "        if p2_peaks.size > 0:\n",
    "            p2_peak_idx = idx_p2_window[p2_peaks[0]]\n",
    "            p2_latency = times[p2_peak_idx]\n",
    "            p2_amplitude = data[p2_peak_idx]\n",
    "        else:\n",
    "            p2_latency, p2_amplitude = np.nan, np.nan\n",
    "\n",
    "        return p2_latency, p2_amplitude\n",
    "\n",
    "    else:\n",
    "        # Detect both N1 and P2 peaks\n",
    "        # Detect local minima (N1) within the specified time range\n",
    "        idx_n1_window = np.where((times >= tmin_n1) & (times <= tmax_n1))[0]\n",
    "        data_n1_window = data[idx_n1_window]\n",
    "        n1_peaks, _ = find_peaks(-data_n1_window)  # Negate data to find minima\n",
    "        if n1_peaks.size > 0:\n",
    "            n1_peak_idx = idx_n1_window[n1_peaks[0]]\n",
    "            n1_latency = times[n1_peak_idx]\n",
    "            n1_amplitude = data[n1_peak_idx]\n",
    "        else:\n",
    "            n1_latency, n1_amplitude = np.nan, np.nan\n",
    "\n",
    "        # Detect local maxima (P2) within the specified time range\n",
    "        idx_p2_window = np.where((times >= tmin_p2) & (times <= tmax_p2))[0]\n",
    "        data_p2_window = data[idx_p2_window]\n",
    "        p2_peaks, _ = find_peaks(data_p2_window)\n",
    "        if p2_peaks.size > 0:\n",
    "            p2_peak_idx = idx_p2_window[p2_peaks[0]]\n",
    "            p2_latency = times[p2_peak_idx]\n",
    "            p2_amplitude = data[p2_peak_idx]\n",
    "        else:\n",
    "            p2_latency, p2_amplitude = np.nan, np.nan\n",
    "\n",
    "        # Detect local maxima (P2) within the specified time range\n",
    "        idx_p3b_window = np.where((times >= tmin_p3b) & (times <= tmax_p3b))[0]\n",
    "        data_p3b_window = data[idx_p3b_window]\n",
    "        p3b_peaks, _ = find_peaks(data_p3b_window)\n",
    "        if p3b_peaks.size > 0:\n",
    "            p3b_peak_idx = idx_p3b_window[p3b_peaks[0]]\n",
    "            p3b_latency = times[p3b_peak_idx]\n",
    "            p3b_amplitude = data[p3b_peak_idx]\n",
    "        else:\n",
    "            p3b_latency, p3b_amplitude = np.nan, np.nan\n",
    "        \n",
    "        return n1_latency, n1_amplitude, p2_latency, p2_amplitude, p3b_latency, p3b_amplitude\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Grand averages from ppts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your preprocessed EEG data for each subject and calculate grand average\n",
    "subject_files = glob.glob('cleaned_seg_child/*.vhdr')  # Get all EEG files in the 'cleaned_seg_child' folder (update this path as needed)\n",
    "skip = []  # List of participant IDs to skip\n",
    "run = []   # (Optional) List of specific participants to run, currently not used\n",
    "\n",
    "# Initialize lists to store participant data and results\n",
    "PID = []  \n",
    "overall_evoked = []  \n",
    "close_evoked = []  \n",
    "mid_evoked = []  \n",
    "far_evoked = []  \n",
    "overall_evoked_P8 = [] \n",
    "close_evoked_P8 = [] \n",
    "mid_evoked_P8 = []  \n",
    "far_evoked_P8 = []  \n",
    "peaks_list = []  #\n",
    "mean_amp_list = [] \n",
    "evoked_df = pd.DataFrame()  \n",
    "seg_length = pd.DataFrame()  \n",
    "no_p8 = []  \n",
    "\n",
    "# Loop through all EEG files\n",
    "for file in subject_files:\n",
    "    # Extract participant ID from the filename by removing specific parts of the string\n",
    "    id = file.replace('cleaned_seg_child/', \"\").replace('_test_Artifact Rejection 1.vhdr', \"\").replace('_Test_Artifact Rejection 1.vhdr', \"\").replace('_baseline_Artifact Rejection 1.vhdr', \"\")\n",
    "\n",
    "    # Skip files if the participant ID is in the 'skip' list\n",
    "    if id in skip:\n",
    "        print(f'Skipping {file} as it was in the skip subject list')\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            PID.append(id)  # Store the participant ID\n",
    "            print(f'Processing participant: {id}')\n",
    "\n",
    "            # Load the raw EEG data using the BrainVision format\n",
    "            raw = mne.io.read_raw_brainvision(file, preload=True, verbose='Warning')\n",
    "            # Optionally, apply a filter (commented out)\n",
    "            # raw = raw.filter(l_freq=0.1, h_freq=30, verbose='Warning')\n",
    "\n",
    "            # Extract events and labels from the raw data (assuming a custom function 'update_events')\n",
    "            events, labels = update_events(raw)\n",
    "\n",
    "            # Set the montage (electrode positions) for the EEG data\n",
    "            montage = mne.channels.make_standard_montage(\"easycap-M1\")\n",
    "            raw.set_montage(montage, verbose='Warning')\n",
    "\n",
    "            # Create an empty event dictionary to map condition names to event codes\n",
    "            event_dict = {}\n",
    "            if 'Stimulus/target_close' in labels:\n",
    "                event_dict['target_close'] = labels['Stimulus/target_close']\n",
    "            if 'Stimulus/target_mid' in labels:\n",
    "                event_dict['target_mid'] = labels['Stimulus/target_mid']\n",
    "            if 'Stimulus/target_far' in labels:\n",
    "                event_dict['target_far'] = labels['Stimulus/target_far']\n",
    "\n",
    "            # Create epochs (time-locked segments) based on the events with a baseline correction\n",
    "            epochs = mne.Epochs(raw, events, event_id=event_dict, tmin=-0.2, tmax=0.5, preload=True, baseline=(-0.2, 0), verbose='Warning')\n",
    "\n",
    "            # Check if the channel 'P8' is present in the data\n",
    "            if 'P8' in epochs.info['ch_names']:\n",
    "                overall_evoked_P8.append(epochs.average())  # Append average evoked response for all conditions at P8\n",
    "            else:\n",
    "                no_p8.append(id)  # Store participant ID if P8 is missing\n",
    "\n",
    "            overall_evoked.append(epochs.average())  # Append the overall evoked response for all conditions\n",
    "\n",
    "            # Initialize a dictionary to store the number of epochs for each condition\n",
    "            evoked_dict = {'PID': id}\n",
    "            \n",
    "            # For 'close' condition\n",
    "            if 'Stimulus/target_close' in labels:\n",
    "                close_evoked.append(epochs['target_close'].average())  # Append average evoked response for 'close' condition\n",
    "                if 'P8' in epochs.info['ch_names']:\n",
    "                    close_evoked_P8.append(epochs['target_close'].average())  # Append evoked response at P8 for 'close' condition\n",
    "                evoked_dict['close'] = len(epochs['target_close'])  # Store the number of epochs in 'close' condition\n",
    "            else:\n",
    "                evoked_dict['close'] = 0  # If no 'close' condition, set it to 0\n",
    "\n",
    "            # For 'mid' condition\n",
    "            if 'Stimulus/target_mid' in labels:\n",
    "                mid_evoked.append(epochs['target_mid'].average())  # Append average evoked response for 'mid' condition\n",
    "                if 'P8' in epochs.info['ch_names']:\n",
    "                    mid_evoked_P8.append(epochs['target_mid'].average())  # Append evoked response at P8 for 'mid' condition\n",
    "                evoked_dict['mid'] = len(epochs['target_mid'])  # Store the number of epochs in 'mid' condition\n",
    "            else:\n",
    "                evoked_dict['mid'] = 0  # If no 'mid' condition, set it to 0\n",
    "\n",
    "            # For 'far' condition\n",
    "            if 'Stimulus/target_far' in labels:\n",
    "                far_evoked.append(epochs['target_far'].average())  # Append average evoked response for 'far' condition\n",
    "                if 'P8' in epochs.info['ch_names']:\n",
    "                    far_evoked_P8.append(epochs['target_far'].average())  # Append evoked response at P8 for 'far' condition\n",
    "                evoked_dict['far'] = len(epochs['target_far'])  # Store the number of epochs in 'far' condition\n",
    "            else:\n",
    "                evoked_dict['far'] = 0  # If no 'far' condition, set it to 0\n",
    "\n",
    "            # Convert evoked data dictionary to a DataFrame and concatenate with the overall segment length DataFrame\n",
    "            evoked_dict = pd.DataFrame(evoked_dict, index=[0])\n",
    "            seg_length = pd.concat([seg_length, evoked_dict], ignore_index=True)\n",
    "\n",
    "        except Exception as error:\n",
    "            # Handle exceptions, raise them to stop the script, and print the error and filename\n",
    "            raise\n",
    "            print(\"An exception occurred:\", error)\n",
    "            print(file)\n",
    "\n",
    "# Calculate the grand average evoked responses for each condition across participants\n",
    "overall_grand_average = mne.grand_average(overall_evoked)\n",
    "close_grand_average = mne.grand_average(close_evoked)\n",
    "mid_grand_average = mne.grand_average(mid_evoked)\n",
    "far_grand_average = mne.grand_average(far_evoked)\n",
    "\n",
    "# Calculate the grand average evoked responses for each condition across participants at P8 channel\n",
    "overall_grand_average_P8 = mne.grand_average(overall_evoked_P8)\n",
    "close_grand_average_P8 = mne.grand_average(close_evoked_P8)\n",
    "mid_grand_average_P8 = mne.grand_average(mid_evoked_P8)\n",
    "far_grand_average_P8 = mne.grand_average(far_evoked_P8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERP and Topo maps joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topo_grand_average = overall_grand_average_P8.copy()\n",
    "topo_grand_average.drop_channels(['Fp1', 'Fp2'])\n",
    "overall_topo = topo_grand_average.plot_topomap([-0.2, 0, 0.15, 0.25, 0.375, 0.5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3 and P4 grand averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set time windows to extact ERPs\n",
    "tmin_n1 = 0.13\n",
    "tmax_n1 = 0.22\n",
    "tmin_p2 = 0.21\n",
    "tmax_p2 = 0.28\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Copy grand average data frames for each channel;\n",
    "p3_grand_average = overall_grand_average.copy()\n",
    "p4_grand_average = overall_grand_average.copy()\n",
    "\n",
    "# get timesa and data for P3 and P4 channels\n",
    "p3_times = p3_grand_average.pick(['P3']).times\n",
    "p3_data = p3_grand_average.pick(['P3']).data[0]\n",
    "\n",
    "p4_times = p4_grand_average.pick(['P4']).times\n",
    "p4_data = p4_grand_average.pick(['P4']).data[0]\n",
    "\n",
    "\n",
    "# Plot the P3 and P4 ERPs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(p3_times, p3_data, label='P3 Grand Average')\n",
    "plt.plot(p4_times, p4_data, label='P4Grand Average')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude (µV)')\n",
    "plt.legend()\n",
    "# Mark the N1 region\n",
    "plt.axvspan(tmin_n1, tmax_n1, color='blue', alpha=0.1, label='N1 Region')\n",
    "\n",
    "\n",
    "# Mark the P2 region\n",
    "plt.axvspan(tmin_p2, tmax_p2, color='red', alpha=0.1, label='P2 Region')\n",
    "# Mark the detected P2 peak\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set time windows to extact ERPs\n",
    "tmin_n1 = 0.13\n",
    "tmax_n1 = 0.22\n",
    "tmin_p2 = 0.21\n",
    "tmax_p2 = 0.28\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Copy grand average data frames for each channel\n",
    "p7_grand_average = overall_grand_average_P8.copy()\n",
    "p8_grand_average = overall_grand_average_P8.copy()\n",
    "\n",
    "# get timesa and data for P7 and P8 channels\n",
    "p7_times = p7_grand_average.pick(['P7']).times\n",
    "p7_data = p7_grand_average.pick(['P7']).data[0]\n",
    "\n",
    "p8_times = p8_grand_average.pick(['P8']).times\n",
    "p8_data = p8_grand_average.pick(['P8']).data[0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(p7_times, p7_data, label='P3 Grand Average')\n",
    "plt.plot(p8_times, p8_data, label='P4Grand Average')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude (µV)')\n",
    "plt.legend()\n",
    "# Mark the N1 region\n",
    "plt.axvspan(tmin_n1, tmax_n1, color='blue', alpha=0.1, label='N1 Region')\n",
    "\n",
    "\n",
    "# Mark the P2 region\n",
    "plt.axvspan(tmin_p2, tmax_p2, color='red', alpha=0.1, label='P2 Region')\n",
    "# Mark the detected P2 peak\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Field Power plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_gfp = overall_grand_average.plot(gfp=True, spatial_colors=True, ylim=dict(eeg=[-12, 12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evoked plots for selcted elctrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_picks = [\"P3\", \"P7\", \"O1\"]\n",
    "right_picks = [\"P4\", \"P8\", \"O2\"]\n",
    "\n",
    "overall_left_plot = overall_grand_average.plot(picks=left_picks)\n",
    "overall_right_plot = overall_grand_average.plot(picks=right_picks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect peaks using a local minima/maxima from scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your preprocessed EEG data for each subject and calculate grand average\n",
    "subject_files = glob.glob('cleaned_seg_child/*.vhdr')  # Update with your actual file path\n",
    "skip = []# Initialize an empty list to store peak data for each participant and condition\n",
    "run = []\n",
    "\n",
    "# Define time windows for N1, P2, and P3b components\n",
    "tmin_n1 = 0.13\n",
    "tmax_n1 = 0.22\n",
    "tmin_p2 = 0.21\n",
    "tmax_p2 = 0.28\n",
    "tmin_p3b = 0.25\n",
    "tmax_p3b = 0.5\n",
    "\n",
    "peaks_list = []  # List to store detected peak data\n",
    "\n",
    "# Loop through each EEG file (participant)\n",
    "for file in subject_files:\n",
    "\n",
    "    # Extract participant ID from the filename\n",
    "    id = file.replace('cleaned_seg_child/', \"\").replace('_test_Artifact Rejection 1.vhdr', \"\").replace('_Test_Artifact Rejection 1.vhdr', \"\").replace('_baseline_Artifact Rejection 1.vhdr', \"\")\n",
    "\n",
    "    # Skip participants in the skip list\n",
    "    if id in skip:\n",
    "        print(f'Skipping {file} as it was in the skip subject list')\n",
    "        continue  \n",
    "\n",
    "    else:\n",
    "        PID.append(id)  # Store participant ID\n",
    "\n",
    "        # Load the raw EEG data\n",
    "        print(f'Processing participant: {id}')\n",
    "        raw = mne.io.read_raw_brainvision(file, preload=True, verbose='Warning')\n",
    "        events, labels = update_events(raw)\n",
    "\n",
    "        # Set montage (electrode placement) for the EEG data\n",
    "        montage = mne.channels.make_standard_montage(\"easycap-M1\")\n",
    "        raw.set_montage(montage, verbose='Warning')\n",
    "\n",
    "        # Create an event dictionary for different stimulus conditions\n",
    "        event_dict = {}\n",
    "        if 'Stimulus/target_close' in labels:\n",
    "            event_dict['target_close'] = labels['Stimulus/target_close']\n",
    "        if 'Stimulus/target_mid' in labels:\n",
    "            event_dict['target_mid'] = labels['Stimulus/target_mid']\n",
    "        if 'Stimulus/target_far' in labels:\n",
    "            event_dict['target_far'] = labels['Stimulus/target_far']\n",
    "\n",
    "        # Create epochs for the EEG data for each event\n",
    "        epochs = mne.Epochs(raw, events, event_id=event_dict, tmin=-0.2, tmax=0.7, preload=True, baseline=(None, 0), verbose='Warning')\n",
    "\n",
    "        # Create evoked responses for each condition\n",
    "        evoked_dict = {}\n",
    "        if 'Stimulus/target_close' in labels:\n",
    "            evoked_dict['close'] = epochs['target_close']\n",
    "        if 'Stimulus/target_mid' in labels:\n",
    "            evoked_dict['mid'] = epochs['target_mid']\n",
    "        if 'Stimulus/target_far' in labels:\n",
    "            evoked_dict['far'] = epochs['target_far']\n",
    "\n",
    "        # Loop through each condition and channel to detect peaks\n",
    "        for condition, evoked in evoked_dict.items():\n",
    "            for channel in ['P3', 'P4', 'P7', 'P8']:\n",
    "                \n",
    "                # Skip specific case for participant 117 and channel P8\n",
    "                if (id == '117' and channel == 'P8'):\n",
    "                    continue\n",
    "\n",
    "                # Set different time windows for central (P3, P4) vs lateral (P7, P8) channels\n",
    "                if channel in ['P3', 'P4']:\n",
    "                    tmin_n1, tmax_n1 = 0.12, 0.18\n",
    "                    tmin_p2, tmax_p2 = 0.18, 0.25\n",
    "                else:\n",
    "                    tmin_n1, tmax_n1 = 0.13, 0.19\n",
    "                    tmin_p2, tmax_p2 = 0.19, 0.25\n",
    "                \n",
    "                # Get the evoked response for the selected channel and detect peaks\n",
    "                evoked_channel = evoked.copy().average().pick([channel])\n",
    "                n1_latency, n1_amplitude, p2_latency, p2_amplitude, p3b_latency, p3b_amplitude = detect_peaks(\n",
    "                    evoked_channel, tmin_n1=tmin_n1, tmax_n1=tmax_n1, tmin_p2=tmin_p2, tmax_p2=tmax_p2, tmin_p3b=tmin_p3b, tmax_p3b=tmax_p3b)\n",
    "\n",
    "                # Store detected peak data in the list\n",
    "                peaks_list.append({\n",
    "                    'PID': id,\n",
    "                    'Condition': condition,\n",
    "                    'Channel': channel,\n",
    "                    'N1_Latency': n1_latency,\n",
    "                    'N1_Amplitude': n1_amplitude,\n",
    "                    'P2_Latency': p2_latency,\n",
    "                    'P2_Amplitude': p2_amplitude,\n",
    "                    'P3b_Latency': p3b_latency,\n",
    "                    'P3b_Amplitude': p3b_amplitude\n",
    "                })\n",
    "\n",
    "# Convert the list of peaks to a DataFrame for further analysis\n",
    "original_peaks_df = pd.DataFrame(peaks_list)\n",
    "# Save the DataFrame to a CSV file if needed\n",
    "# original_peaks_df.to_csv('child_subjects_peaks.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot comparisons for each condition P3 and P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "# Define the electrode picks for left (P3) and right (P4) hemispheres\n",
    "right_picks = ['P4']\n",
    "left_picks = ['P3']\n",
    "\n",
    "# Pick left hemisphere channel (P3) and combine across conditions using the average\n",
    "left_ix = mne.pick_channels(overall_grand_average.info[\"ch_names\"], include=left_picks)\n",
    "overall_roi_left = mne.channels.combine_channels(overall_grand_average, {'left_ROI':left_ix}, method=\"mean\")\n",
    "close_roi_left = mne.channels.combine_channels(close_grand_average, {'left_ROI':left_ix}, method=\"mean\")\n",
    "mid_roi_left = mne.channels.combine_channels(mid_grand_average, {'left_ROI':left_ix}, method=\"mean\")\n",
    "far_roi_left = mne.channels.combine_channels(far_grand_average, {'left_ROI':left_ix}, method=\"mean\")\n",
    "\n",
    "# Pick right hemisphere channel (P4) and combine across conditions using the average\n",
    "right_ix = mne.pick_channels(overall_grand_average.info[\"ch_names\"], include=right_picks)\n",
    "overall_roi_right = mne.channels.combine_channels(overall_grand_average, {'right_ROI':right_ix}, method=\"mean\")\n",
    "close_roi_right = mne.channels.combine_channels(close_grand_average, {'right_ROI':right_ix}, method=\"mean\")\n",
    "mid_roi_right = mne.channels.combine_channels(mid_grand_average, {'right_ROI':right_ix}, method=\"mean\")\n",
    "far_roi_right = mne.channels.combine_channels(far_grand_average, {'right_ROI':right_ix}, method=\"mean\")\n",
    "\n",
    "# Combine evoked responses for left hemisphere across conditions for comparison\n",
    "full_left_grand_averages = dict(close=close_roi_left, mid=mid_roi_left, far=far_roi_left)\n",
    "full_left_compare_plot = mne.viz.plot_compare_evokeds(full_left_grand_averages)\n",
    "\n",
    "# Combine evoked responses for right hemisphere across conditions for comparison\n",
    "full_right_grand_averages = dict(close=close_roi_right, mid=mid_roi_right, far=far_roi_right)\n",
    "full_right_compare_plot = mne.viz.plot_compare_evokeds(full_right_grand_averages)\n",
    "\n",
    "# Plot a subset of conditions (close and mid) for left hemisphere\n",
    "left_grand_averages = dict(close=close_roi_left, mid=mid_roi_left)\n",
    "left_compare_plot = mne.viz.plot_compare_evokeds(left_grand_averages)\n",
    "\n",
    "# Plot a subset of conditions (close and mid) for right hemisphere\n",
    "right_grand_averages = dict(close=close_roi_right, mid=mid_roi_right)\n",
    "right_compare_plot = mne.viz.plot_compare_evokeds(right_grand_averages)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison plots for P7 and P8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "# Define the electrode picks for left (P8) and right (P7) hemispheres\n",
    "right_picks = ['P7']\n",
    "left_picks = ['P8']\n",
    "\n",
    "# Pick left hemisphere channels (P8) and combine across conditions using the mean\n",
    "left_ix = mne.pick_channels(overall_grand_average_P8.info[\"ch_names\"], include=left_picks)\n",
    "overall_roi_left = mne.channels.combine_channels(overall_grand_average_P8, {'left_ROI':left_ix}, method=\"mean\")\n",
    "close_roi_left = mne.channels.combine_channels(close_grand_average_P8, {'left_ROI':left_ix}, method=\"mean\")\n",
    "mid_roi_left = mne.channels.combine_channels(mid_grand_average_P8, {'left_ROI':left_ix}, method=\"mean\")\n",
    "far_roi_left = mne.channels.combine_channels(far_grand_average_P8, {'left_ROI':left_ix}, method=\"mean\")\n",
    "\n",
    "# Pick right hemisphere channels (P7) and combine across conditions using the mean\n",
    "right_ix = mne.pick_channels(overall_grand_average_P8.info[\"ch_names\"], include=right_picks)\n",
    "overall_roi_right = mne.channels.combine_channels(overall_grand_average_P8, {'right_ROI':right_ix}, method=\"mean\")\n",
    "close_roi_right = mne.channels.combine_channels(close_grand_average_P8, {'right_ROI':right_ix}, method=\"mean\")\n",
    "mid_roi_right = mne.channels.combine_channels(mid_grand_average_P8, {'right_ROI':right_ix}, method=\"mean\")\n",
    "far_roi_right = mne.channels.combine_channels(far_grand_average_P8, {'right_ROI':right_ix}, method=\"mean\")\n",
    "\n",
    "# Combine evoked responses for left hemisphere across all conditions and plot them\n",
    "full_left_grand_averages = dict(close=close_roi_left, mid=mid_roi_left, far=far_roi_left)\n",
    "full_left_compare_plot = mne.viz.plot_compare_evokeds(full_left_grand_averages)\n",
    "\n",
    "# Combine evoked responses for right hemisphere across all conditions and plot them\n",
    "full_right_grand_averages = dict(close=close_roi_right, mid=mid_roi_right, far=far_roi_right)\n",
    "full_right_compare_plot = mne.viz.plot_compare_evokeds(full_right_grand_averages)\n",
    "\n",
    "# Plot a subset of conditions (close and mid) for left hemisphere\n",
    "left_grand_averages = dict(close=close_roi_left, mid=mid_roi_left)\n",
    "left_compare_plot = mne.viz.plot_compare_evokeds(left_grand_averages)\n",
    "\n",
    "# Plot a subset of conditions (close and mid) for right hemisphere\n",
    "right_grand_averages = dict(close=close_roi_right, mid=mid_roi_right)\n",
    "right_compare_plot = mne.viz.plot_compare_evokeds(right_grand_averages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA removing Far condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_df = original_peaks_df[original_peaks_df['Condition'] != 'far']\n",
    "peaks_df = peaks_df.loc[peaks_df['Channel'].isin(['P3', 'P4'])]\n",
    "\n",
    "# Add Hemisphere column based on Channel\n",
    "peaks_df['Hemisphere'] = peaks_df['Channel'].map({'P3': 'Left', 'P4': 'Right'})\n",
    "\n",
    "\n",
    "# Perform ANOVA for N1 Latency\n",
    "anova_n1_latency = pg.rm_anova(dv='N1_Latency', within=['Condition', 'Hemisphere'], subject='PID', data=peaks_df)\n",
    "print(\"ANOVA Results for N1 Latency:\")\n",
    "print(anova_n1_latency)\n",
    "\n",
    "# Perform ANOVA for N1 Amplitude\n",
    "anova_n1_amplitude = pg.rm_anova(dv='N1_Amplitude', within=['Condition', 'Hemisphere'], subject='PID', data=peaks_df)\n",
    "print(\"ANOVA Results for N1 Amplitude:\")\n",
    "print(anova_n1_amplitude)\n",
    "\n",
    "# Perform ANOVA for P2 Latency\n",
    "anova_p2_latency = pg.rm_anova(dv='P2_Latency', within=['Condition', 'Hemisphere'], subject='PID', data=peaks_df)\n",
    "print(\"ANOVA Results for P2 Latency:\")\n",
    "print(anova_p2_latency)\n",
    "\n",
    "# Perform ANOVA for P2 Amplitude\n",
    "anova_p2_amplitude = pg.rm_anova(dv='P2_Amplitude', within=['Condition', 'Hemisphere'], subject='PID', data=peaks_df)\n",
    "print(\"ANOVA Results for P2 Amplitude:\")\n",
    "print(anova_p2_amplitude)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
