{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup file\n",
    "\n",
    "Import files and set up custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from scipy.signal import find_peaks\n",
    "import pingouin as pg\n",
    "\n",
    "import mne\n",
    "\n",
    "# Function to update event codes\n",
    "def update_events(raw):\n",
    "    events, labels = mne.events_from_annotations(raw, verbose='Warning')\n",
    "\n",
    "    target_onset_trigger = labels['Comment/Time 0/']\n",
    "    trigger_1 = labels['Stimulus/S  1']\n",
    "    trigger_2 = labels['Stimulus/S  2']\n",
    "    trigger_3 = labels['Stimulus/S  3']\n",
    "    trigger_5 = labels['Stimulus/S  5']\n",
    "    trigger_6 = labels['Stimulus/S  6']\n",
    "    trigger_7 = labels['Stimulus/S  7']\n",
    "\n",
    "    num_trig = [trigger_1, trigger_2, trigger_3, trigger_5, trigger_6, trigger_7]\n",
    "\n",
    "    for i in range(len(events)):\n",
    "        if events[i, 2] == target_onset_trigger:\n",
    "            j = i\n",
    "            while True:\n",
    "                j += 1\n",
    "                if events[j, 2] in num_trig:\n",
    "                    if events[j, 2] in [trigger_1, trigger_7]:\n",
    "                        events[i, 2] = 300\n",
    "                    elif events[j, 2] in [trigger_2, trigger_6]:\n",
    "                        events[i, 2] = 200\n",
    "                    elif events[j, 2] in [trigger_3, trigger_5]:\n",
    "                        events[i, 2] = 100\n",
    "                    break\n",
    "\n",
    "    labels['Stimulus/target_close'] = 100\n",
    "    labels['Stimulus/target_mid'] = 200\n",
    "    labels['Stimulus/target_far'] = 300\n",
    "\n",
    "    return events, labels\n",
    "\n",
    "\n",
    "\n",
    "    # Function to detect local peaks using find_peaks\n",
    "def detect_peaks(evoked, tmin_n1=None, tmax_n1=None, tmin_p2=None, tmax_p2=None, tmin_p3b=None, tmax_p3b=None, peak_to_detect=None):\n",
    "    times = evoked.times\n",
    "    data = evoked.data[0, :]\n",
    "    \n",
    "    if peak_to_detect == 'N1':\n",
    "        # Detect local minima (N1) within the specified time range\n",
    "        idx_n1_window = np.where((times >= tmin_n1) & (times <= tmax_n1))[0]\n",
    "        data_n1_window = data[idx_n1_window]\n",
    "        n1_peaks, _ = find_peaks(-data_n1_window)  # Negate data to find minima\n",
    "        if n1_peaks.size > 0:\n",
    "            n1_peak_idx = idx_n1_window[n1_peaks[0]]\n",
    "            n1_latency = times[n1_peak_idx]\n",
    "            n1_amplitude = data[n1_peak_idx]\n",
    "        else:\n",
    "            n1_latency, n1_amplitude = np.nan, np.nan\n",
    "\n",
    "        return n1_latency, n1_amplitude\n",
    "\n",
    "    elif peak_to_detect == 'P2':\n",
    "        # Detect local maxima (P2) within the specified time range\n",
    "        idx_p2_window = np.where((times >= tmin_p2) & (times <= tmax_p2))[0]\n",
    "        data_p2_window = data[idx_p2_window]\n",
    "        p2_peaks, _ = find_peaks(data_p2_window)\n",
    "        if p2_peaks.size > 0:\n",
    "            p2_peak_idx = idx_p2_window[p2_peaks[0]]\n",
    "            p2_latency = times[p2_peak_idx]\n",
    "            p2_amplitude = data[p2_peak_idx]\n",
    "        else:\n",
    "            p2_latency, p2_amplitude = np.nan, np.nan\n",
    "\n",
    "        return p2_latency, p2_amplitude\n",
    "\n",
    "    else:\n",
    "        # Detect both N1 and P2 peaks\n",
    "        # Detect local minima (N1) within the specified time range\n",
    "        idx_n1_window = np.where((times >= tmin_n1) & (times <= tmax_n1))[0]\n",
    "        data_n1_window = data[idx_n1_window]\n",
    "        n1_peaks, _ = find_peaks(-data_n1_window)  # Negate data to find minima\n",
    "        if n1_peaks.size > 0:\n",
    "            n1_peak_idx = idx_n1_window[n1_peaks[0]]\n",
    "            n1_latency = times[n1_peak_idx]\n",
    "            n1_amplitude = data[n1_peak_idx]\n",
    "        else:\n",
    "            n1_latency, n1_amplitude = np.nan, np.nan\n",
    "\n",
    "        # Detect local maxima (P2) within the specified time range\n",
    "        idx_p2_window = np.where((times >= tmin_p2) & (times <= tmax_p2))[0]\n",
    "        data_p2_window = data[idx_p2_window]\n",
    "        p2_peaks, _ = find_peaks(data_p2_window)\n",
    "        if p2_peaks.size > 0:\n",
    "            p2_peak_idx = idx_p2_window[p2_peaks[0]]\n",
    "            p2_latency = times[p2_peak_idx]\n",
    "            p2_amplitude = data[p2_peak_idx]\n",
    "        else:\n",
    "            p2_latency, p2_amplitude = np.nan, np.nan\n",
    "\n",
    "        # Detect local maxima (P2) within the specified time range\n",
    "        idx_p3b_window = np.where((times >= tmin_p3b) & (times <= tmax_p3b))[0]\n",
    "        data_p3b_window = data[idx_p3b_window]\n",
    "        p3b_peaks, _ = find_peaks(data_p3b_window)\n",
    "        if p3b_peaks.size > 0:\n",
    "            p3b_peak_idx = idx_p3b_window[p3b_peaks[0]]\n",
    "            p3b_latency = times[p3b_peak_idx]\n",
    "            p3b_amplitude = data[p3b_peak_idx]\n",
    "        else:\n",
    "            p3b_latency, p3b_amplitude = np.nan, np.nan\n",
    "        \n",
    "        return n1_latency, n1_amplitude, p2_latency, p2_amplitude, p3b_latency, p3b_amplitude\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Grand averages from ppts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed EEG data for each subject and calculate grand averages\n",
    "subject_files = glob.glob('clean_seg_updated/*.vhdr')  # File path for EEG data\n",
    "skip = []  # List of files/participants to skip\n",
    "run = []   # Optional list of participants to include\n",
    "\n",
    "# Initialize lists to store participant IDs and evoked responses\n",
    "PID = []\n",
    "overall_evoked = []\n",
    "close_evoked = []\n",
    "mid_evoked = []\n",
    "far_evoked = []\n",
    "peaks_list = []\n",
    "mean_amp_list = []\n",
    "evoked_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each EEG file (participant)\n",
    "for file in subject_files:\n",
    "\n",
    "    # Skip files in the 'skip' list\n",
    "    if file in skip:\n",
    "        print(f'Skipping {file} as it was in the skip subject list')\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        # Extract participant ID from the filename and store it\n",
    "        id = file.replace('clean_seg_updated/', \"\").replace('_seg_corrected_raw.vhdr', \"\")\n",
    "        PID.append(id)\n",
    "        print(f'Processing participant: {id}')\n",
    "\n",
    "        # Try reading the raw data up to 3 times in case of errors\n",
    "        try_count = 0\n",
    "        while try_count < 3:\n",
    "            try:\n",
    "                raw = mne.io.read_raw_brainvision(file, preload=True, verbose='Warning')\n",
    "                try_count += 1\n",
    "            except:\n",
    "                print('Unable to read bva file, trying again')\n",
    "\n",
    "        # Set montage (electrode positions) for the EEG data\n",
    "        events, labels = update_events(raw)\n",
    "        montage = mne.channels.make_standard_montage(\"easycap-M1\")\n",
    "        raw = raw.set_montage(montage, verbose='Warning')\n",
    "\n",
    "        # Define event dictionary for different stimulus conditions\n",
    "        event_dict = {\n",
    "            'target_close': labels['Stimulus/target_close'], \n",
    "            'target_mid': labels['Stimulus/target_mid'], \n",
    "            'target_far': labels['Stimulus/target_far']\n",
    "        }\n",
    "\n",
    "        # Create epochs (segments) for the EEG data based on events\n",
    "        epochs = mne.Epochs(raw, events, event_id=event_dict, tmin=-0.2, tmax=0.8, preload=True, baseline=(-0.2, 0), verbose='Warning')\n",
    "        print(len(epochs))  # Print the number of epochs\n",
    "\n",
    "        # Append the average evoked response for overall and each condition\n",
    "        overall_evoked.append(epochs.average())\n",
    "        close_evoked.append(epochs['target_close'].average())\n",
    "        mid_evoked.append(epochs['target_mid'].average())\n",
    "        far_evoked.append(epochs['target_far'].average())\n",
    "\n",
    "        # Store evoked data in a dictionary for later use\n",
    "        evoked_dict = dict(PID=id, close=epochs['target_close'], mid=epochs['target_mid'], far=epochs['target_far'])\n",
    "\n",
    "# Calculate grand averages for overall and each condition\n",
    "overall_grand_average = mne.grand_average(overall_evoked)\n",
    "close_grand_average = mne.grand_average(close_evoked)\n",
    "mid_grand_average = mne.grand_average(mid_evoked)\n",
    "far_grand_average = mne.grand_average(far_evoked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERP and Topo maps joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_joint = overall_grand_average.plot_joint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_grand_average = overall_grand_average.copy()\n",
    "topo_grand_average.drop_channels(['Fp1', 'Fp2'])\n",
    "overall_topo = topo_grand_average.plot_topomap([-0.2, 0, 0.15, 0.25, 0.375, 0.5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3 and P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time windows for N1 and P2 components\n",
    "tmin_n1 = 0.12  # N1 start time\n",
    "tmax_n1 = 0.18  # N1 end time\n",
    "tmin_p2 = 0.18  # P2 start time\n",
    "tmax_p2 = 0.25  # P2 end time\n",
    "\n",
    "# Enable inline plotting for Jupyter notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Copy the grand average data for channels P3 and P4\n",
    "p3_grand_average = overall_grand_average.copy()\n",
    "p4_grand_average = overall_grand_average.copy()\n",
    "\n",
    "# Extract time and amplitude data for P3 and P4 channels\n",
    "p3_times = p3_grand_average.pick(['P3']).times\n",
    "p3_data = p3_grand_average.pick(['P3']).data[0]\n",
    "p4_times = p4_grand_average.pick(['P4']).times\n",
    "p4_data = p4_grand_average.pick(['P4']).data[0]\n",
    "\n",
    "# Plot the grand average waveforms for P3 and P4\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(p3_times, p3_data, label='P3 Grand Average')\n",
    "plt.plot(p4_times, p4_data, label='P4 Grand Average')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude (µV)')\n",
    "plt.legend()\n",
    "\n",
    "# Highlight the N1 and P2 regions with shaded areas\n",
    "plt.axvspan(tmin_n1, tmax_n1, color='blue', alpha=0.1, label='N1 Region')\n",
    "plt.axvspan(tmin_p2, tmax_p2, color='red', alpha=0.1, label='P2 Region')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time windows for N1 and P2 components\n",
    "tmin_n1 = 0.13  # N1 start time\n",
    "tmax_n1 = 0.19  # N1 end time\n",
    "tmin_p2 = 0.19  # P2 start time\n",
    "tmax_p2 = 0.25  # P2 end time\n",
    "\n",
    "# Enable inline plotting for Jupyter notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Copy the grand average data for channels P7 and P8\n",
    "p7_grand_average = overall_grand_average.copy()\n",
    "p8_grand_average = overall_grand_average.copy()\n",
    "\n",
    "# Extract time and amplitude data for P7 and P8 channels\n",
    "p7_times = p7_grand_average.pick(['P7']).times\n",
    "p7_data = p7_grand_average.pick(['P7']).data[0]\n",
    "p8_times = p8_grand_average.pick(['P8']).times\n",
    "p8_data = p8_grand_average.pick(['P8']).data[0]\n",
    "\n",
    "# Plot the grand average waveforms for P7 and P8\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(p7_times, p7_data, label='P7 Grand Average')\n",
    "plt.plot(p8_times, p8_data, label='P8 Grand Average')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude (µV)')\n",
    "plt.legend()\n",
    "\n",
    "# Highlight the N1 and P2 regions with shaded areas\n",
    "plt.axvspan(tmin_n1, tmax_n1, color='blue', alpha=0.1, label='N1 Region')\n",
    "plt.axvspan(tmin_p2, tmax_p2, color='red', alpha=0.1, label='P2 Region')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Field Power plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_gfp = overall_grand_average.plot(gfp=True, spatial_colors=True, ylim=dict(eeg=[-12, 12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evoked plots for selcted elctrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_picks = [\"P3\", \"P7\", \"O1\"]\n",
    "right_picks = [\"P4\", \"P8\", \"O2\"]\n",
    "\n",
    "overall_left_plot = overall_grand_average.plot(picks=left_picks)\n",
    "overall_right_plot = overall_grand_average.plot(picks=right_picks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect peaks using a local minima/maxima from scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of EEG files to process\n",
    "subject_files = glob.glob('clean_seg_updated/*.vhdr')  # Update with actual file path\n",
    "skip = []  # List of files/participants to skip\n",
    "run = []   # Optional list of participants to include\n",
    "\n",
    "# Define time windows for N1, P2, and P3b components\n",
    "tmin_n1 = 0.12\n",
    "tmax_n1 = 0.18\n",
    "tmin_p2 = 0.18\n",
    "tmax_p2 = 0.25\n",
    "tmin_p3b = 0.25\n",
    "tmax_p3b = 0.5\n",
    "\n",
    "peaks_list = []  # List to store detected peak data\n",
    "\n",
    "# Loop through each EEG file (participant)\n",
    "for file in subject_files:\n",
    "\n",
    "    # Skip files in the 'skip' list\n",
    "    if file in skip:\n",
    "        print(f'Skipping {file} as it was in the skip subject list')\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        # Extract participant ID from the filename and store it\n",
    "        id = file.replace('clean_seg_updated/', \"\").replace('_seg_corrected_raw.vhdr', \"\")\n",
    "        PID.append(id)\n",
    "        print(f'Processing participant: {id}')\n",
    "\n",
    "        # Try reading the raw data up to 3 times in case of errors\n",
    "        try_count = 0\n",
    "        while try_count < 3:\n",
    "            try:\n",
    "                raw = mne.io.read_raw_brainvision(file, preload=True, verbose='Warning')\n",
    "                try_count += 1\n",
    "            except:\n",
    "                print('Unable to read bva file, trying again')\n",
    "\n",
    "        # Update the events and labels from the raw EEG data\n",
    "        events, labels = update_events(raw)\n",
    "\n",
    "        # Set the montage (electrode positions) for the EEG data\n",
    "        montage = mne.channels.make_standard_montage(\"easycap-M1\")\n",
    "        raw.set_montage(montage, verbose='Warning')\n",
    "\n",
    "        # Create an event dictionary for different stimulus conditions\n",
    "        event_dict = {\n",
    "            'target_close': labels['Stimulus/target_close'], \n",
    "            'target_mid': labels['Stimulus/target_mid'], \n",
    "            'target_far': labels['Stimulus/target_far']\n",
    "        }\n",
    "\n",
    "        # Create epochs (segments) for the EEG data based on events\n",
    "        epochs = mne.Epochs(raw, events, event_id=event_dict, tmin=-0.2, tmax=0.7, preload=True, baseline=(None, 0), verbose='Warning')\n",
    "\n",
    "        # Store evoked responses for each condition (close, mid, far)\n",
    "        evoked_dict = dict(close=epochs['target_close'], mid=epochs['target_mid'], far=epochs['target_far'])\n",
    "\n",
    "        # Loop through conditions and channels to detect peaks\n",
    "        for condition, evoked in evoked_dict.items():\n",
    "            for channel in ['P3', 'P4', 'P7', 'P8']:\n",
    "\n",
    "                # Define time windows based on channel (different for P3/P4 vs P7/P8)\n",
    "                if channel in ['P3', 'P4']:\n",
    "                    tmin_n1 = 0.12\n",
    "                    tmax_n1 = 0.18\n",
    "                    tmin_p2 = 0.18\n",
    "                    tmax_p2 = 0.25\n",
    "                    tmin_p3b = 0.25\n",
    "                    tmax_p3b = 0.5\n",
    "                else:\n",
    "                    tmin_n1 = 0.13\n",
    "                    tmax_n1 = 0.19\n",
    "                    tmin_p2 = 0.19\n",
    "                    tmax_p2 = 0.25  \n",
    "                    tmin_p3b = 0.25\n",
    "                    tmax_p3b = 0.5\n",
    "\n",
    "                # Detect peaks (N1, P2, P3b) in the evoked response for the channel\n",
    "                evoked_channel = evoked.copy().average().pick([channel])\n",
    "                n1_latency, n1_amplitude, p2_latency, p2_amplitude, p3b_latency, p3b_amplitude = detect_peaks(\n",
    "                    evoked_channel, tmin_n1=tmin_n1, tmax_n1=tmax_n1, tmin_p2=tmin_p2, tmax_p2=tmax_p2, tmin_p3b=tmin_p3b, tmax_p3b=tmax_p3b)\n",
    "\n",
    "                # Append detected peaks to the list\n",
    "                peaks_list.append({\n",
    "                    'PID': id,\n",
    "                    'Condition': condition,\n",
    "                    'Channel': channel,\n",
    "                    'N1_Latency': n1_latency,\n",
    "                    'N1_Amplitude': n1_amplitude,\n",
    "                    'P2_Latency': p2_latency,\n",
    "                    'P2_Amplitude': p2_amplitude,\n",
    "                    'P3b_Latency': p3b_latency,\n",
    "                    'P3b_Amplitude': p3b_amplitude\n",
    "                })\n",
    "\n",
    "# Convert the list of peaks into a DataFrame and save it to a CSV file\n",
    "original_peaks_df = pd.DataFrame(peaks_list)\n",
    "original_peaks_df.to_csv('all_subjects_peaks.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ANOVA on peaks for P3 and P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "peaks_df = original_peaks_df.copy()\n",
    "peaks_df = peaks_df.loc[~peaks_df['PID'].isin(['A016', 'A025', 'A013', 'A014'])]\n",
    "peaks_df = peaks_df.loc[peaks_df['Channel'].isin(['P3','P4'])]\n",
    "\n",
    "# Add Hemisphere column based on Channel\n",
    "peaks_df['Hemisphere'] = peaks_df['Channel'].map({'P3': 'Left', 'P4': 'Right'})\n",
    "# means_df['Hemisphere'] = means_df['Channel'].map({'P3': 'Left', 'P4': 'Right'})\n",
    "\n",
    "# Perform ANOVA for N1 Latency\n",
    "anova_n1_latency = pg.rm_anova(data=peaks_df, dv='N1_Latency', within=['Condition', 'Hemisphere'], subject='PID')\n",
    "print(\"ANOVA Results for N1 Latency:\")\n",
    "print(anova_n1_latency)\n",
    "\n",
    "# Perform ANOVA for N1 Amplitude\n",
    "anova_n1_amplitude = pg.rm_anova(dv='N1_Amplitude', within=['Condition', 'Hemisphere'], subject='PID', data=peaks_df)\n",
    "print(\"ANOVA Results for N1 Amplitude:\")\n",
    "print(anova_n1_amplitude)\n",
    "\n",
    "# Perform ANOVA for P2 Latency\n",
    "anova_p2_latency = pg.rm_anova(dv='P2_Latency', within=['Condition', 'Hemisphere'], subject='PID', data=peaks_df)\n",
    "print(\"ANOVA Results for P2 Latency:\")\n",
    "print(anova_p2_latency)\n",
    "\n",
    "# Perform ANOVA for P2 Amplitude\n",
    "anova_p2_amplitude = pg.rm_anova(dv='P2_Amplitude', within=['Condition', 'Hemisphere'], subject='PID', data=peaks_df)\n",
    "print(\"ANOVA Results for P2 Amplitude:\")\n",
    "print(anova_p2_amplitude)\n",
    "\n",
    "# Perform post-hoc tests using pairwise comparisons\n",
    "post_hoc_results_N1 = pg.pairwise_ttests(dv='N1_Amplitude', \n",
    "                                      within=['Condition', 'Channel'], \n",
    "                                      subject='PID', \n",
    "                                      data=peaks_df, \n",
    "                                      padjust='bonferroni')\n",
    "print(post_hoc_results_N1)\n",
    "\n",
    "# Perform post-hoc tests using pairwise comparisons\n",
    "post_hoc_results_N1_lat = pg.pairwise_ttests(dv='N1_Latency', \n",
    "                                      within=['Condition', 'Channel'], \n",
    "                                      subject='PID', \n",
    "                                      data=peaks_df, \n",
    "                                      padjust='bonferroni')\n",
    "print(post_hoc_results_N1_lat)\n",
    "\n",
    "# Perform post-hoc tests using pairwise comparisons\n",
    "post_hoc_results_P2 = pg.pairwise_ttests(dv='P2_Amplitude', \n",
    "                                      within=['Condition', 'Channel'], \n",
    "                                      subject='PID', \n",
    "                                      data=peaks_df, \n",
    "                                      padjust='bonferroni')\n",
    "print(post_hoc_results_P2)\n",
    "\n",
    "anova_n1_latency['Measure'] = 'N1_lat'\n",
    "anova_p2_latency['Measure'] = 'P2_lat'\n",
    "\n",
    "anova_n1_amplitude['Measure'] = 'N1_amp'\n",
    "anova_p2_amplitude['Measure'] = 'P2_amp'\n",
    "\n",
    "anova_output = pd.concat([anova_n1_latency, anova_n1_amplitude, anova_p2_latency, anova_p2_amplitude])\n",
    "\n",
    "first_column = anova_output.pop('Measure')\n",
    "anova_output.insert(0, 'Measure', first_column) \n",
    "\n",
    "# anova_output.to_csv('anova_output.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
